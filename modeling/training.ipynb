{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe64b7d-96ce-4ea8-a620-52f7a39de88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "ROOT_DIR = '/'.join(os.getcwd().split('/')[:-1])\n",
    "os.chdir(ROOT_DIR)\n",
    "from modeling.modeling_utils import *\n",
    "import json\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "WARMUP_PROPORTION = 0.1\n",
    "local_rank = -1\n",
    "no_cuda = False\n",
    "seed = 2022\n",
    "output_dir = f'{ROOT_DIR}output/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "local_test = False\n",
    "## model folder named 'pretraining' should be in the root folder\n",
    "\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True, add_sampler=False): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    if add_sampler == True:\n",
    "        logger.info('add sampler')\n",
    "        if local_rank == -1:\n",
    "            sampler = SequentialSampler(data)\n",
    "        else:\n",
    "            sampler = DistributedSampler(data)\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=EVAL_BATCH)\n",
    "    else:\n",
    "        dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "def train(Classifier, NUM_TRAIN_EPOCHS):\n",
    "    total_train_loss_history = []\n",
    "    total_val_loss_history = []\n",
    "    total_learning_rate = []\n",
    "     \n",
    "    for real_epoch in trange(int(NUM_TRAIN_EPOCHS)):\n",
    "        Classifier.train()\n",
    "        tr_loss_epoch, train_loss_history, learning_rate_list, Classifier = train_one_epoch(Classifier, real_epoch)\n",
    "        total_train_loss_history.extend(train_loss_history)\n",
    "        total_learning_rate.extend(learning_rate_list)\n",
    "        result, loss_history = evaluate(epoch, Classifier)\n",
    "        total_val_loss_history.extend(loss_history)\n",
    "        logging.info(result)\n",
    "        model_name = f'clinicalbert_LEARNING_RATE_{LEARNING_RATE}_gradient_accu_{GRADIENT_ACCUMULATION_STEPS}_MAX_GRAD_NORM_{MAX_GRAD_NORM}_{str(real_epoch)}.pt'\n",
    "        torch.save(Classifier.state_dict(), f'{output_dir}{model_name}')\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    plt.plot(total_train_loss_history)\n",
    "    plt.plot(total_val_loss_history)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_one_epoch(Classifier, epoch):\n",
    "    global_step = 0 \n",
    "    global_step_check = 0\n",
    "    no_improvement = 0\n",
    "    train_loss_history = []\n",
    "    tr_loss = 0 \n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f'Training iteration: {str(epoch)}')):\n",
    "        Classifier.to(device)\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        #with torch.set_grad_enabled(True):\n",
    "        loss, logits = Classifier(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids)\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        loss.backward() \n",
    "        train_loss_history.append(loss.item())\n",
    "        tr_loss += loss.item()\n",
    "        if ((step + 1) % GRADIENT_ACCUMULATION_STEPS == 0  or (step+1) == len(train_dataloader)):\n",
    "            torch.nn.utils.clip_grad_norm_(Classifier.parameters(), MAX_GRAD_NORM)  \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            Classifier.zero_grad()\n",
    "            global_step += 1\n",
    "        if (step+1) % 200 == 0:\n",
    "            logging_str =  \"***** epoch [{}]\".format(epoch)\n",
    "            logging_str += \" global_step [{}]\".format(global_step) \n",
    "            logging_str += \" train loss [{}]\".format(loss.item())  \n",
    "            logging.info(logging_str)\n",
    "        nb_tr_steps = nb_tr_steps + 1\n",
    "    tr_loss_epoch = tr_loss / nb_tr_steps\n",
    "    learning_rate_list = scheduler.get_lr()\n",
    "    return tr_loss_epoch, train_loss_history, learning_rate_list, Classifier\n",
    "            \n",
    "            \n",
    "def evaluate(epoch, Classifier):\n",
    "    nb_eval_examples = 0\n",
    "    nb_eval_steps = 0\n",
    "    m = nn.Sigmoid()\n",
    "    Classifier.eval()\n",
    "    eval_loss = 0\n",
    "    eval_accuracy = 0\n",
    "    nb_eval_steps = 0\n",
    "    pred_labels, true_labels, logits_history, pred_scores = [], [], [], []   \n",
    "    loss_history = []\n",
    "    dev_example = processor.get_dev_examples(f'{ROOT_DIR}/data/3days/')\n",
    "    \n",
    "    dev_features = convert_examples_to_features(\n",
    "            dev_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "    dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, EVAL_BATCH, shuffle=False)\n",
    "    for step, batch in enumerate(tqdm(dev_dataloader, desc=\"Evaluation iteration: {str(epoch)}'\")): \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss, temp_logits = Classifier(input_ids, attention_mask=input_mask,\n",
    "                                    token_type_ids=segment_ids, labels=label_ids)\n",
    "            logits = Classifier(input_ids,segment_ids,input_mask) # we don't need twice\n",
    "        \n",
    "        logits = torch.squeeze(m(logits)).detach().cpu().numpy()    \n",
    "        label_ids = np.array(np.array(label_ids.to('cpu')))\n",
    "        outputs = np.asarray([1 if i else 0 for i in (logits.flatten()>=0.5)])\n",
    "        tmp_eval_accuracy=np.sum(outputs == label_ids)    \n",
    "        \n",
    "        true_labels += list(label_ids)\n",
    "        pred_labels += list(outputs)\n",
    "        logits_history = logits_history + logits.flatten().tolist()\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        loss_history.append(tmp_eval_loss.item())\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "            \n",
    "    logging_str =  \"***** epoch [{}]\".format(epoch)\n",
    "    logging_str += \" {} [{:.4}]\".format('val_loss', eval_loss)\n",
    "    \n",
    "    df_test = pd.read_csv(f'{ROOT_DIR}/data/3days/val.csv')\n",
    "    fpr, tpr, df_out, roc_auc = vote_score(df_test, logits_history, output_dir)\n",
    "    pr_auc = pr_curve_plot(df_test['Label'].values, logits_history, output_dir)\n",
    "    rp80 =  vote_pr_curve(df_test, logits_history, output_dir)\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,  \n",
    "              'training loss': tr_loss_epoch,\n",
    "              'AUROC': roc_auc,\n",
    "              'AUPRC' : pr_auc,\n",
    "              'RP80': rp80}     \n",
    "    return result, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5ce9f-c4e5-400e-a630-dc93851a85f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bert_utils import BertForSequenceClassification\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import json\n",
    "local_rank  = 0\n",
    "local_test = False\n",
    "LEARNING_RATE=5e-05\n",
    "file_path = f\"{ROOT_DIR}/secrets.json\"\n",
    "if local_test:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "else:\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        proxies = json_data['proxies']\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', proxies=proxies) \n",
    "processors = {\n",
    "    \"readmission\": readmissionProcessor\n",
    "}\n",
    "processor = processors['readmission']()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "train_examples = processor.get_train_examples(f'{ROOT_DIR}/data/3days/')\n",
    "test_example = processor.get_test_examples(f'{ROOT_DIR}/data/3days/')\n",
    "dev_example = processor.get_dev_examples(f'{ROOT_DIR}/data/3days/')\n",
    "train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = convert_examples_to_features(\n",
    "        test_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "dev_features = convert_examples_to_features(\n",
    "        dev_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, EVAL_BATCH, shuffle=False)\n",
    "test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, EVAL_BATCH, shuffle=False)\n",
    "\n",
    "if local_rank == -1 or no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    n_gpu = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "## number of loss backward? \n",
    "num_train_steps = int(len(train_dataloader.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)   \n",
    "num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n",
    "Classifier = BertForSequenceClassification.from_pretrained(os.path.join(f'{ROOT_DIR}/', 'pretraining'), 1)\n",
    "Classifier.to(device)\n",
    "param_optimizer = list(Classifier.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=LEARNING_RATE, warmup=WARMUP_PROPORTION, t_total=num_train_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "MAX_GRAD_NORM = 3\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "total_train_loss_history = []\n",
    "total_val_loss_history = []\n",
    "total_learning_rate = []\n",
    "m = nn.Sigmoid()\n",
    "for real_epoch in trange(int(NUM_TRAIN_EPOCHS)):\n",
    "    Classifier.train()\n",
    "    tr_loss_epoch, train_loss_history, learning_rate_list, Classifier = train_one_epoch(Classifier, real_epoch)\n",
    "    total_train_loss_history.extend(train_loss_history)\n",
    "    total_learning_rate.extend(learning_rate_list)\n",
    "    result, loss_history = evaluate(real_epoch, Classifier)\n",
    "    total_val_loss_history.extend(loss_history)\n",
    "    logging.info(result)\n",
    "    model_name = f'clinicalbert_LEARNING_RATE_{LEARNING_RATE}_gradient_accu_{GRADIENT_ACCUMULATION_STEPS}_MAX_GRAD_NORM_{MAX_GRAD_NORM}_{str(real_epoch)}.pt'\n",
    "    torch.save(Classifier.state_dict(), f'{output_dir}{model_name}')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a13d0b-42e0-43d3-a976-8eba8c7281c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    plt.plot(total_train_loss_history)\n",
    "    plt.plot(total_val_loss_history)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35308aa-ba14-4bfa-b1e7-3ad91dcb7446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
